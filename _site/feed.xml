<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="4.2.0">Jekyll</generator><link href="/feed.xml" rel="self" type="application/atom+xml" /><link href="/" rel="alternate" type="text/html" /><updated>2021-03-25T10:27:27+01:00</updated><id>/feed.xml</id><title type="html">Finite Simulations</title><subtitle>An amazing website.</subtitle><author><name>Vikram Singh</name><email>vikram.singh@mpimet.mpg.de</email></author><entry><title type="html">Can we make machine learning more efficient using conservation laws?</title><link href="/blog_cons_laws_ml/" rel="alternate" type="text/html" title="Can we make machine learning more efficient using conservation laws?" /><published>2021-03-25T00:00:00+01:00</published><updated>2021-03-25T00:00:00+01:00</updated><id>/blog_cons_laws_ml</id><content type="html" xml:base="/blog_cons_laws_ml/">&lt;p&gt;There are many differen ways to do machine learning. 
The most popular methods use neural networks.
We are usually given some data and we train the neural network to
fit this data. 
In this post I will review an interesting way to look at this problem
that converts the training problem to an ordinary differential equation.
We will look at a property of this ODE that indicates a 
possible way to optimize the training process.&lt;/p&gt;

&lt;p&gt;The training of machine learning models require optimizing 
an objective function.
For example, using Tensorflow, you could define a loss function
in the following manner. 
This is simply least squares.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;import keras.backend as K

def loss(y_true,y_pred):
      return K.mean( K.square( y_pred - y_true) )
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Then you select an optimization function such as ADAM and 
then wait for the learning to happen.
But what is really happening when you do this.
Well, most popular optimization functions are really stochastic gradient descent.
Maybe I will talk of stochastic gradient descent in a future post,
but right now I am more interested in discussing gradient descent
and its connections to differential equations and conservation laws.&lt;/p&gt;

&lt;p&gt;Let us first look at how gradient descent works.
First, define the parameters of the neural network, 
namely the weights and biases as the vector \(x\),
and the loss function, for example the one above, as \(V\).
What we want to do is find the minimum of this function \(V\)
and we use gradient descent for this.
Then gradient descent is simply&lt;/p&gt;

\[x^{n + 1} = x^{n} - \gamma \nabla V(x^n).\]

&lt;p&gt;Here \(\nabla V\) is the gradient of \(V\).
This is an iterative algorithm that states how the parameters
at step \(n+1\) should be updated using parameters at step \(n\)
and \(\gamma\) is called learning rate for machine learning.
The gradients in packages such as Tensorflow are done using 
automatic differentiation. The iterations in general go on until the loss function
is sufficiently small.
However, an interesting way to look at this algorithm is
to think of Euler time stepping.
You are given an equation&lt;/p&gt;

\[\dot{x} = - \nabla V(x).\]

&lt;p&gt;Here \(\dot{x}\) is the time derivative of the parameters.
This is called gradient flow.
There’s an excellent blog post&lt;sup id=&quot;fnref:1&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:1&quot; class=&quot;footnote&quot;&gt;1&lt;/a&gt;&lt;/sup&gt;
that goes into more detail.
This is now simply an ordinary differential equation(ODE)
although a very big one!&lt;/p&gt;

&lt;p&gt;But once we realize it is an ODE there are
some new ways to think of this problem.
First, we realize that we are doing Euler time stepping
and to people familiar with numerical methods 
that is usually a strict no-no.
Its usually not stable and restricts you to
using very small sized steps.
Therefore, it is possible that gradient descent is not 
very efficient especially for large problems.
Second, this suggests that we could come up
with some kind of convergence proof for the algorithm.
Of course then we have to start making assumptions about
the function \(V\) but I guess having guarantees
on how efficiently we can do learning for a machine learning problem
would be a sort of Holy Grail.
So this is a very active area of research.
And third, we can start thinking about conservation laws
that the ODE may satisfy and investigate whether the algorithm
satisfies this law.
The second and third point is interconnected but I have separated them
because I want to talk more about this third point.&lt;/p&gt;

&lt;p&gt;In computational fluid dynamics(CFD) which is what I work on,
conservation laws are everywhere.
In CFD, we want to conserve mass, momentum and energy,
which is what the conservation laws tell you to do.
However, when approximating the equations, we often
don’t guarantee conservation resulting in bad results or the solver 
crashing. 
However, even if you do satisfy conservation, it turns out 
that the flow satisfies additional laws.
For example, the compressible Navier Stokes satisfy entropy 
conservation and it turns out if you satisfy them in your approximation
your solver becomes much more stable in general. 
My last post on wall models explored one such stability aspect.
Similarly for gradient flows there’s a conservation law,
or rather a stability law that is satisfied.
To see this, we just multiply \(\nabla V\) to the ODE to get&lt;/p&gt;

\[\nabla V \cdot \dot{x} = - (\nabla V)^2.\]

&lt;p&gt;Now we simply use the chain rule \(\frac{dV}{dx}\frac{dx}{dt} = \frac{dV}{dt}\)
to get&lt;/p&gt;

\[\dot{V} = - (\nabla V)^2.\]

&lt;p&gt;Notice that the right hand side is always negative, meaning
that \(V\) will always decrease which is what we want since
we want its minimum.
But does gradient descent do this?
We multiply \(\nabla V(x^n)\) with the gradient descent equation to get&lt;/p&gt;

\[\nabla V(x^n)
\frac{x^{n + 1} - x^{n}}{\gamma} = - ( \nabla V(x^n) )^2.\]

&lt;p&gt;Notice that the left hand side is only an approximation
for \(\dot{V}\).
As \(\gamma\) becomes bigger this approximation gets worse
(using Taylor series)
and so you cannot have large learning rates.&lt;/p&gt;

&lt;p&gt;One way to solve this issue would be to define
the gradient in a different way.
This approach is called the discrete gradient approach and 
the earliest reference I found was a paper from the 70’s&lt;sup id=&quot;fnref:2&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:2&quot; class=&quot;footnote&quot;&gt;2&lt;/a&gt;&lt;/sup&gt;.
The first two authors of the paper are 2 giants of CFD
and of course I should not have been surprised.
Recent studies I have found usually  explore
applications in image&lt;sup id=&quot;fnref:3&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:3&quot; class=&quot;footnote&quot;&gt;3&lt;/a&gt;&lt;/sup&gt; regularization&lt;sup id=&quot;fnref:4&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:4&quot; class=&quot;footnote&quot;&gt;4&lt;/a&gt;&lt;/sup&gt;. 
In this approach we define the gradient in the following manner.&lt;/p&gt;

\[\bar{\nabla} V(x, y)
(x - y)= V(x) - V(y).\]

&lt;p&gt;Now, if we use this definition of the gradient, then 
our iterative algorithm will in fact satisfy the stability condition.
Some people familiar with CFD will recognize that this condition
looks very similar to the famous Tadmor shuffle condition &lt;sup id=&quot;fnref:5&quot; role=&quot;doc-noteref&quot;&gt;&lt;a href=&quot;#fn:5&quot; class=&quot;footnote&quot;&gt;5&lt;/a&gt;&lt;/sup&gt;.
There also, we define a function such that we can satisfy a conservation law.
However, there are 2 issues that makes this new definition difficult 
to use.
First, we cannot use automatic differentiation anymore which could affect efficiency.
And second, this then becomes an implicit equation.
For example, gradient descent will look like the following&lt;/p&gt;

\[x^{n + 1} = x^{n} - \gamma \bar{\nabla} V(x^n, x^{n+1}).\]

&lt;p&gt;So, we have an implicit equation, which looks like
implicit time stepping used in CFD. 
Obviously we don’t have something as simple as Euler time stepping,
but here’s something that CFD tells us. 
If we do implicit time stepping, we can usually get away with much larger 
time steps. I suspect the same is true for this problem as well.
So, if we come up with efficient ways to do the implicit time stepping,
we can do training with a much larger learning rate. 
This would make it much more efficient.&lt;/p&gt;

&lt;p&gt;So that is it. 
My aim with the post was to summarize some of the ideas 
in literature that allows people from the numerical analysis community
to get familiar with the problem. 
I think this is an interesting direction to attack the problem
and come up with solutions to make training more efficient.
Don’t hesistate to contact me if you have any questions or suggestions.&lt;/p&gt;

&lt;h3 id=&quot;references&quot;&gt;References&lt;/h3&gt;

&lt;div class=&quot;footnotes&quot; role=&quot;doc-endnotes&quot;&gt;
  &lt;ol&gt;
    &lt;li id=&quot;fn:1&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://francisbach.com/gradient-flows/&quot;&gt;https://francisbach.com/gradient-flows/&lt;/a&gt; &lt;a href=&quot;#fnref:1&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:2&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://doi.org/10.1002/cpa.3160310205&quot;&gt;https://doi.org/10.1002/cpa.3160310205&lt;/a&gt; &lt;a href=&quot;#fnref:2&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:3&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://doi.org/10.1088/1751-8121/aa747c&quot;&gt;https://doi.org/10.1088/1751-8121/aa747c&lt;/a&gt; &lt;a href=&quot;#fnref:3&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:4&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1805.06444.pdf&quot;&gt;https://arxiv.org/pdf/1805.06444.pdf&lt;/a&gt; &lt;a href=&quot;#fnref:4&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
    &lt;li id=&quot;fn:5&quot; role=&quot;doc-endnote&quot;&gt;
      &lt;p&gt;&lt;a href=&quot;https://doi.org/10.2307/2008251&quot;&gt;https://doi.org/10.2307/2008251&lt;/a&gt; &lt;a href=&quot;#fnref:5&quot; class=&quot;reversefootnote&quot; role=&quot;doc-backlink&quot;&gt;&amp;#8617;&lt;/a&gt;&lt;/p&gt;
    &lt;/li&gt;
  &lt;/ol&gt;
&lt;/div&gt;</content><author><name>Vikram Singh</name><email>vikram.singh@mpimet.mpg.de</email></author><summary type="html">There are many differen ways to do machine learning. The most popular methods use neural networks. We are usually given some data and we train the neural network to fit this data. In this post I will review an interesting way to look at this problem that converts the training problem to an ordinary differential equation. We will look at a property of this ODE that indicates a possible way to optimize the training process.</summary></entry><entry><title type="html">Are wall models stable?</title><link href="/blog_paper_wall_model/" rel="alternate" type="text/html" title="Are wall models stable?" /><published>2021-03-16T00:00:00+01:00</published><updated>2021-03-16T00:00:00+01:00</updated><id>/blog_paper_wall_model</id><content type="html" xml:base="/blog_paper_wall_model/">&lt;p&gt;The second paper from my PhD got published recently in 
&lt;a href=&quot;https://www.sciencedirect.com/science/article/abs/pii/S0045793021000360&quot;&gt;Computers and Fluids&lt;/a&gt;.
If you don’t have access, there’s a free version on
&lt;a href=&quot;https://www.researchgate.net/publication/349396252_Impact_of_wall_modeling_on_kinetic_energy_stability_for_the_compressible_Navier-Stokes_equations&quot;&gt;ResearchGate&lt;/a&gt;
and the preprint on
&lt;a href=&quot;https://arxiv.org/abs/2102.05080&quot;&gt;Arxiv&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;The idea of the paper was to look at wall models from an angle that
is not usually looked at.
Wall models are a popular way to reduce the cost of simulations.
To see why let us first say something about accurate ways of solving
for flow around walls, such as say the wing of an airplane.
Direct numerical simulation(DNS) is the most
accurate way to solve the Navier Stokes equations,
but if we have a wall then the cost of simulations is crazy expensive.
I think the Channel Flow test case, which is very idealized took
9 months to solve on a supercomputer for a realistic Reynolds’ number.
So usually people look at the next most accurate way of doing it,
which is large eddy simulation(LES).
However, the wall still poses a problem since the cost is
still exponenetial.
I recall a talk where people were using billions of grid points
to just solve the flow around a car. And I don’t think they were even doing LES.&lt;/p&gt;

&lt;p&gt;Hence wall models. The idea is to do LES away from the wall, 
and use a simple model near it.
The grid need not be so refined then and so we can get away
with doing simulations at much lower cost. 
However, stability is not usually something that is looked at when 
using wall models even though it is very important.
Without stability, you submit a simulation overnight, hoping to see results in the morning
and instead you come back to see that the solver stopped barely after starting.&lt;/p&gt;

&lt;p&gt;We looked at kinetic energy \(\frac{1}{2} \rho u^2\)(\(\rho\) being density and \(u\) being velocity) 
to see the effects of the wall model on it.
We proved that a simple relation at the wall decides the stability.
Using this, we looked at a couple of wall models.
It turns out that the algebraic wall model, which is very popular,
is actually unstable.
Which is surprising, not only because its so popular but because it
often gives fairly accurate results.
We created an idealized test case to demonstrate this issue
on an airfoil.
So next time you simulate flow around a wall, and start seeing
robustness issues, maybe the wall model is the issue.
On the other hand a recently developed slip wall model is actually stable,
but is not accurate.
So, we decided to test one that is a hybrid of the two.
It worked fairly well but we need to push the accuracy forward.&lt;/p&gt;

&lt;p&gt;I am happy at how the paper turned out and am really thankful to 
Prof. Nordström for pushing us to investigate this direction 
as well as his detailed revisions of the manuscript
and to Prof. Frankel for all his help.
Have a look at the paper and if you have any questions or suggestions
feel free to send me a message.&lt;/p&gt;</content><author><name>Vikram Singh</name><email>vikram.singh@mpimet.mpg.de</email></author><summary type="html">The second paper from my PhD got published recently in Computers and Fluids. If you don’t have access, there’s a free version on ResearchGate and the preprint on Arxiv.</summary></entry><entry><title type="html">Setting up the blog</title><link href="/blog_setup/" rel="alternate" type="text/html" title="Setting up the blog" /><published>2021-02-01T00:00:00+01:00</published><updated>2021-02-01T00:00:00+01:00</updated><id>/blog_setup</id><content type="html" xml:base="/blog_setup/">&lt;p&gt;Hello world!!&lt;/p&gt;

&lt;p&gt;I thought I will start of by posting on how I setup this blog.
First I decided to use 
&lt;a href=&quot;https://pages.github.com/&quot;&gt;GitHub Pages&lt;/a&gt;
since I already had a Github account and it sounded simple to set it up.
Not simple at all as it turned out.
While there are many posts around with headlines like
setup your blog in 5 minutes, the reality is that 
it takes 5 minutes if you want to setup an exact replica of a pre-existing template.
Figure out how to do even the most simple customizations can take quite some time.
Let us begin then.&lt;/p&gt;

&lt;h2 id=&quot;first-steps&quot;&gt;First steps&lt;/h2&gt;

&lt;p&gt;I decided to use Jekyll since that is what Github recommends.
Now, Jekyll requires Ruby. A version of Ruby is already available on Mac. 
However, I had some issues, so I used Homebrew to install Ruby.
If you don’t have Ruby, please install it for your platform. 
Next, create a directory where you will keep the website files and 
in the terminal do the following inside this directory.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;gem install bundler jekyll
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Now, we could use the steps listed at various steps such as
&lt;a href=&quot;https://www.freecodecamp.org/news/create-a-free-static-site-with-github-pages-in-10-minutes/&quot;&gt;here&lt;/a&gt;
to use the default theme.
But I decided to use the 
&lt;a href=&quot;https://github.com/mmistakes/minimal-mistakes&quot;&gt;Minimal Mistakes&lt;/a&gt;
theme which looked better.
Download the repository to your folder and then inside the folder do&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bundle update
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And you are done! You can view your website by doing&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;bundle exec jekyll serve  
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and then putting the following in your browser.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;http://localhost:4000/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;That took 5 minutes. So what’s the problem.
Well, quite a few of them. But first, the easy part.
Use the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_config.yml&lt;/code&gt; to change things such as the name of the website,
your social media links etc. 
Every time, you make a change, you can do the above steps to open it in the browser
and check the results.
Its exactly like compiling and running!&lt;/p&gt;

&lt;h2 id=&quot;customize&quot;&gt;Customize&lt;/h2&gt;

&lt;p&gt;The very first thing I wanted to do was figure out how to add
a page for blog posts.
To do this add a folder &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_pages&lt;/code&gt; in the home directory
and inside this directory add a file called &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;blog.md&lt;/code&gt;.
Write the following inside this file&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;---
title:  &quot;Blog&quot;
layout: archive
permalink: /Blog/
author_profile: true
---

&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Next edit the file &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_data/navigation.yml&lt;/code&gt; to look like this.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;main:
  - title: &quot;Blog&quot;
    url: /Blog/
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;This completes the setup. To write posts, create a new folder
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_posts&lt;/code&gt; and add blog posts there. The file must be named like this.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;yyyy-mm-dd-name.md
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;The top of the file must have&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;---
layout: single
title:  &quot;Name&quot;
date:   yyyy-mm-dd
---
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;and then start writing your post below. Note that if you name the files with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;.md&lt;/code&gt; 
extension, you are basically telling Jekyll that you are using 
&lt;a href=&quot;https://guides.github.com/features/mastering-markdown/&quot;&gt;Markdown&lt;/a&gt;.
So we are done with setting up blogging.
There’s a slight hitch though, although this is just a personal choice.
With this setup Jekyll puts the posts on your home page. 
I did not want this.
So I deleted the &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;index.html&lt;/code&gt; page in the home directory
and added an &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;index.md&lt;/code&gt; file with the following lines.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;---
title: &apos;About me&apos;
layout: single
author_profile: true
---
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;One more customization I decided to do was to add a newsletter sign-up form.
To do this, I signed up at 
&lt;a href=&quot;https://mailchimp.com/&quot;&gt;Mailchimp&lt;/a&gt;
and created an embedded signup form. 
Mailchimp gave me the HTML for the form which I copy-pasted into a new file
&lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_data/signup.html&lt;/code&gt;.
I also added the following line into &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_layouts/single.html&lt;/code&gt;.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;{# if page.signup == true #}{# include signup.html #}{# endif #}.
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Replace &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;#&lt;/code&gt; with &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;%&lt;/code&gt; in the above. Jekyll thinks I am trying to execute
this from inside the blog post.
I modified the defaults in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_config.yml&lt;/code&gt; to add the signup line.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;defaults:
  - scope:
      path: &quot;&quot;
      type: posts
    values:
      layout: single
      author_profile: true
      share: true
      related: true
      signup: true
      signup: true
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Finally, in &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;_pages/blog.md&lt;/code&gt; I added the following in the header.&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;signup: true
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;Phew!!
And now we need to do the easiest part. 
Put it all online. 
Go to your GitHub account, and setup a new repository &lt;code class=&quot;language-plaintext highlighter-rouge&quot;&gt;username.github.io&lt;/code&gt;.
From the terminal do&lt;/p&gt;

&lt;div class=&quot;language-plaintext highlighter-rouge&quot;&gt;&lt;div class=&quot;highlight&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;git init
git remote add origin git@github.com:&amp;lt;your_github_username&amp;gt;/&amp;lt;your_github_repo_name&amp;gt;.git
git add *
git commit -m &quot;Setting up Jekyll&quot;
git push -u origin master
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;

&lt;p&gt;And we are done.
So that is how I setup my blog. 
Of course I continue to change things here and there. 
If you are interested, have a look at the files on my GitHub.&lt;/p&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;</content><author><name>Vikram Singh</name><email>vikram.singh@mpimet.mpg.de</email></author><summary type="html">Hello world!!</summary></entry></feed>